{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anant1525/BucketPick/blob/main/Spaceship_survival_problem_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FWzYlAJDbTME"
      },
      "outputs": [],
      "source": [
        "# # the Gym environment class\n",
        "from gym import Env # predefined spaces from Gym\n",
        "from gym import spaces # used to randomize starting positions\n",
        "import random # used for integer datatypes\n",
        "import numpy as np\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KVLPLWTlbX_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533bfd9e-b380-44c4-f619-f8fb639de6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class SpaceshipEnvironment(Env):\n",
        "    def __init__(self):\n",
        "        self.cumulative_reward = 0\n",
        "\n",
        "        # set the initial state to a flattened BOARD_SIZExBOARD_SIZE grid with a randomly placed Spaceship and asteroids\n",
        "        self.state = [ [SPACE]*BOARD_LENGTH for _ in range(BOARD_LENGTH)]\n",
        "\n",
        "        self.spaceship_x = random.randrange(0, BOARD_LENGTH)\n",
        "        self.spaceship_y = random.randrange(0, BOARD_LENGTH)\n",
        "        self.state[self.spaceship_y][self.spaceship_x] = SPACESHIP # Update state\n",
        "        # initialize asteroids positions\n",
        "        self.asteroid_x = []\n",
        "        self.asteroid_y = []\n",
        "        for c in range(ASTEROID_COUNT):\n",
        "            x = random.randrange(0, BOARD_LENGTH)\n",
        "            y = random.randrange(0, BOARD_LENGTH)\n",
        "            # make sure spaceship and any asteroid positions overlapping each other\n",
        "            while x == self.spaceship_x and y == self.spaceship_y:\n",
        "                x = random.randrange(0, BOARD_LENGTH)\n",
        "                y = random.randrange(0, BOARD_LENGTH)\n",
        "            self.asteroid_x.append(x)\n",
        "            self.asteroid_y.append(y)\n",
        "            self.state[y][x] = ASTEROID\n",
        "\n",
        "        # convert the python array into a numpy array (needed since Gym expects the state to be this way)\n",
        "        self.state = np.array(self.state, dtype=np.int16)\n",
        "        print(self.state.shape)\n",
        "\n",
        "        # observation space (valid ranges for observations in the state)\n",
        "        self.observation_space = spaces.Box(0, 2, [BOARD_LENGTH, BOARD_LENGTH], dtype=np.int16)\n",
        "\n",
        "        # valid actions:\n",
        "        #   0 = up\n",
        "        #   1 = down\n",
        "        #   2 = left\n",
        "        #   3 = right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "    def step(self, action):\n",
        "        # placeholder for debugging information\n",
        "        info = {}\n",
        "\n",
        "        # set default values for done, reward, and the player position before taking the action\n",
        "        done = False\n",
        "        reward = -0.01\n",
        "        previous_position_x = self.spaceship_x\n",
        "        previous_position_y = self.spaceship_y\n",
        "\n",
        "        # take the action by moving the player\n",
        "        if action == UP:\n",
        "            self.spaceship_y -= 1\n",
        "            if self.spaceship_y < 0:\n",
        "                self.spaceship_y = (BOARD_LENGTH - 1)\n",
        "\n",
        "        elif action == DOWN:\n",
        "            self.spaceship_y += 1\n",
        "            if self.spaceship_y >= BOARD_LENGTH:\n",
        "                self.spaceship_y = 0\n",
        "\n",
        "        elif action == LEFT:\n",
        "            self.spaceship_x -= 1\n",
        "            if self.spaceship_x < 0:\n",
        "                self.spaceship_x = (BOARD_LENGTH - 1)\n",
        "\n",
        "        elif action == RIGHT:\n",
        "            self.spaceship_x += 1\n",
        "            if self.spaceship_x >= BOARD_LENGTH:\n",
        "                self.spaceship_x = 0\n",
        "        else:\n",
        "            print(action)\n",
        "            raise Exception(\"invalid action\")\n",
        "\n",
        "        # check for win/lose conditions and set reward\n",
        "        if self.state[self.spaceship_y][self.spaceship_x] == SPACE:\n",
        "            reward = 1.0\n",
        "            self.cumulative_reward += reward\n",
        "            done = True\n",
        "            clear_screen()\n",
        "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
        "            print('YOU WIN!!!!')\n",
        "\n",
        "        elif self.state[self.spaceship_y][self.spaceship_x] == ASTEROID:\n",
        "            reward = -1.0\n",
        "            self.cumulative_reward += reward\n",
        "            done = True\n",
        "            clear_screen()\n",
        "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
        "            print('YOU LOSE')\n",
        "\n",
        "        #\n",
        "        # Update the environment state\n",
        "        #\n",
        "        if not done:\n",
        "            # update the player position\n",
        "            self.state[previous_position_y][previous_position_x] = SPACE\n",
        "            self.state[self.spaceship_y][self.spaceship_x] = SPACESHIP\n",
        "\n",
        "        self.cumulative_reward += reward\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        # visualization can be added here\n",
        "        pretty_print(self.state, self.cumulative_reward)\n",
        "\n",
        "    def reset(self):\n",
        "        self.cumulative_reward = 0\n",
        "\n",
        "        # set the initial state to a flattened 6x6 grid with a randomly placed entry, win, and player\n",
        "        self.state = [ [SPACE]*BOARD_LENGTH for i in range(BOARD_LENGTH)]\n",
        "\n",
        "        self.spaceship_x = random.randrange(0, BOARD_LENGTH)\n",
        "        self.spaceship_y = random.randrange(0, BOARD_LENGTH)\n",
        "        self.state[self.spaceship_y][self.spaceship_x] = SPACESHIP # Update state\n",
        "        # initialize asteroids positions\n",
        "        self.asteroid_x = []\n",
        "        self.asteroid_y = []\n",
        "        for _ in range(ASTEROID_COUNT):\n",
        "            x = random.randrange(0, BOARD_LENGTH)\n",
        "            y = random.randrange(0, BOARD_LENGTH)\n",
        "            # make sure spaceship and any asteroid positions overlapping each other\n",
        "            while x == self.spaceship_x and y == self.spaceship_y:\n",
        "                x = random.randrange(0, BOARD_LENGTH)\n",
        "                y = random.randrange(0, BOARD_LENGTH)\n",
        "            self.asteroid_x.append(x)\n",
        "            self.asteroid_y.append(y)\n",
        "            self.state[y][x] = ASTEROID\n",
        "\n",
        "        # convert the python array into a numpy array (needed since Gym expects the state to be this way)\n",
        "        self.state = np.array(self.state, dtype=np.int16)\n",
        "\n",
        "        return self.state\n",
        "\n",
        "# clears the screen of any output\n",
        "def clear_screen():\n",
        "    os.system(\"cls\")# prints out the environment state in a visually appealing way\n",
        "def pretty_print(state_array, cumulative_reward):\n",
        "   clear_screen()\n",
        "   print(f'Cumulative Reward: {cumulative_reward}')\n",
        "   print()\n",
        "   for i in range(6):\n",
        "       for j in range(6):\n",
        "           print('{:4}'.format(state_array[i*6 + j]), end = \"\")\n",
        "       print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bNAEEZ61bzB_"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lwe9sSH-sJxs"
      },
      "outputs": [],
      "source": [
        "class CNNQNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, seed):\n",
        "        super(CNNQNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32, 256)  # Adjusted based on input size\n",
        "        self.fc2 = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 32)  # Adjusted based on input size\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "biX8YxNHb-tH"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = CNNQNetwork(state_size, action_size, seed).to(device)\n",
        "        #summary(self.qnetwork_local, state_size)\n",
        "        self.qnetwork_target = CNNQNetwork(state_size, action_size, seed).to(device)\n",
        "        #summary(self.qnetwork_target, state_size)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)  # Convert state to tensor and add batch dimension\n",
        "        print(\"state_tensor : \", state_tensor.shape)\n",
        "        self.qnetwork_local.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state_tensor)\n",
        "\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            #print(\"Action Values : \",action_values)\n",
        "            action = torch.argmax(action_values).item()\n",
        "            msg = \"Best action \"\n",
        "        else:\n",
        "            # Choose a random action\n",
        "            action = random.randint(0, self.action_size - 1)\n",
        "            msg = \"Random action \"\n",
        "\n",
        "        print(\"{0} : {1}\".format( msg, action))\n",
        "        return action\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Reshape states and next_states to have the correct shape for CNN\n",
        "        states = states.view(-1, 1, BOARD_LENGTH, BOARD_LENGTH)\n",
        "        next_states = next_states.view(-1, 1, BOARD_LENGTH, BOARD_LENGTH)\n",
        "\n",
        "        # Compute and minimize the loss\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update the target network\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def locate_spaceship(self, state):\n",
        "        for i, x in enumerate(state):\n",
        "            if 1 in x:\n",
        "                return i, x.index(1)\n",
        "        return 0, 0\n",
        "\n",
        "    def find_best_action(self, state, action_values):\n",
        "        #print(state)\n",
        "        pos = np.where(state==1)\n",
        "        #print(\"pos : \", pos)\n",
        "        y, x = pos[0][0], pos[1][0]\n",
        "        neighbour_action_values = []\n",
        "        neighbour_action_values.append(action_values[y - 1][x]) #up\n",
        "        neighbour_action_values.append(action_values[y + 1 if (y + 1) < BOARD_LENGTH else 0][x])\n",
        "        neighbour_action_values.append(action_values[y][x - 1])\n",
        "        neighbour_action_values.append(action_values[y][x + 1 if (x + 1) < BOARD_LENGTH else 0])\n",
        "        return neighbour_action_values.index(max(neighbour_action_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NGZ2pllxsJxt"
      },
      "outputs": [],
      "source": [
        "def dqn(n_episodes=500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        #print(\"before reset \\n\", env.state)\n",
        "        state = env.reset()\n",
        "        #print(\"after reset \\n\", env.state)\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            #print('action, state, eps\\n', action, state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MKdvnN5_bc6Q"
      },
      "outputs": [],
      "source": [
        "# global constants\n",
        "\n",
        "# board size\n",
        "BOARD_LENGTH = 6\n",
        "BOARD_SIZE = BOARD_LENGTH * BOARD_LENGTH # assuming the board is square\n",
        "\n",
        "# Number of asteroids\n",
        "ASTEROID_COUNT = 4\n",
        "\n",
        "# game board values\n",
        "SPACE = 0\n",
        "SPACESHIP = 1\n",
        "ASTEROID = 2\n",
        "\n",
        "# action values\n",
        "NUM_ACTIONS = 4\n",
        "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlmzC2CKbsdk",
        "outputId": "d752d81f-2974-4b8b-c02c-5afe424f05fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 6)\n"
          ]
        }
      ],
      "source": [
        "env = SpaceshipEnvironment()\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj_-uKo3fZXk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo43x5UxcDi6"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size=(1, BOARD_LENGTH, BOARD_LENGTH), action_size=NUM_ACTIONS, seed=0)\n",
        "scores = dqn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5-A1qNT5-8xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dd8764-b1e9-4b7e-a191-e35397c3f468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}